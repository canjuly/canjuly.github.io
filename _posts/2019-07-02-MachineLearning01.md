---
layout:     post                    # 使用的布局（不需要改）
title:      吴恩达机器学习笔记01                    # 标题 
subtitle:   线性回归                    # 副标题
date:       2019-07-02              # 时间
author:     canjuly                 # 作者
header-img: img/post-bg-SayYou/post-SayYou-1.jpg    # 这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               # 标签
    - 机器学习
    - 笔记
---

## 写在前面
> preview

马上去读研了，导师给了我几个任务，因为读的是智能软件方向，所以老师让我搞搞深度学习什么的。读本科时我没接触过神经网络，所以学习时感到基础不太行，就打算从机器学习的基础开始学起，那么理所应当就打算跟着吴恩达的机器学习和深度学习了。

为了给自己巩固一下，加深印象，就决定做些笔记。

## 监督学习
> supervised learning

机器学习可以分为两大类，一类为监督学习，另外一类是非监督学习。其中监督学习的特征为会有一个**已经标好对应结果**的数据集。比如对于房价问题，训练的数据就应该为：一幢x1平米的，房龄为x2年的房子，价格为y元。而通过学习算法(Learning Algorithm )学习数据集，最终会获得一个函数，也就是我们所需要的预测函数。

## 线性回归
> linear regression

回归问题是属于监督学习的一类问题，通常是根据预测函数来预测一个值，比如上面的房价预测问题就是典型的回归问题。而线性回归，顾名思义，就是预测函数为线性函数的回归问题，即预测函数应表示为如下形式：
```
hθ(x) = θ0+θ1x1+θ2x2+⋯+θnxn
```
显然，所有的变量均为一次项。对于函数hθ(x),θ0,θ1,...,θn即为所要学习的系数，x0,x1,...,xn和结果y为数据集。

## 代价函数
> loss function

回归问题希望找到一条最接近数据集的函数，也就是说，希望预测的θi与真实的θi尽可能接近。为了定义这个概念，我们引入代价函数。

对于线性回归来说，代价函数J(θ)就是一个预测值与真实值的方差.
```
J(θ) = 1/2m * ∑(hθ(x(i))−y(i))²
```
其中m为样本大小，xi，yi为数据集中第i组数据。显然为了使表达更简洁易懂，该函数也可以使用矩阵表达。

值得注意的是，预测函数是一个关于数据x的函数，代价函数是一个关于预测值θ的函数。

## 梯度下降
> Gradient descent

代价函数定义了预测函数的预测值与真实值的差距，显然这个差距越小，拟合的预测函数就越可信，所以我们要找到J(θ)的极小值，此时的θ就是我们的预测结果。

不难发现，要找到J(θ)的极小值只需将其对所有的θi求偏导，令其等于零后可得到一个方程组，解这个方程组即可。但当θ的维度过大时，这显然需要花费大量时间，此时梯度下降算法就是一个不错的选择。

直观的说，梯度下降就是从山上往山下走，通过一种最快的方式走到最低点。对于代价函数J(θ)来说，就是通过迭代的方法更新θ，直到收敛到最小值。梯度下降算法更新θi的方式为：
```
θi = θi − α * ∂J(θ)/∂θi
```
其中α为学习率，容易想到α越大，则收敛的速度越快。但α也不宜过大，因为太大会导致该算法发散。

显然，当θ取到区间极小值时，各θi的偏导数均为0，算法收敛。

## 特征值标准化
> features normalization

由于各特征值的范围相差巨大，故可能导致梯度下降算法收敛速度变慢，此时就需要进行特征值标准化。比如有三个特征值x1,x2,x3,其中-0.01< x1 <0.001,-10< x2 <10,-10000< x3 <10000,那么可令x1'=1000x1,x3'=x3/1000,从而使范围相近，再对x1',x2,x3进行梯度下降算法，这样可以加快算法的收敛速度。

## 正规方程 
> normal equations

正如在上文说的，除了迭代的方法之外，θ可由方程组解出。该方程组有m个变量，m个方程，显然这是一个齐次线性方程组，解法如下：  
![avater](https://www.zhihu.com/equation?tex=%5Ctheta+%3D+%28X%5ETX%29%5E%7B-1%7DX%5ETY)
